# Отчет о лабораторной работе №4

## 1. Постановка задачи

Реализовать метод имитации отжига и метод стахастической оптимизации. Сравнить его с результатами из прошлых лабораторных. 

Эффективность оценивалась на наборе тестовых функций с различными характеристиками (мультимодальные, зашумленные). В качестве ключевых показателей эффективности, согласно заданию, использовались:
*   Количество итераций до достижения критерия остановки.
*   Количество вычислений целевой функции.
*   Количество вычислений градиента.

## 2. Исследование

Исследуемые алгоритмы:

*   Градиентные методы (данные из `optimization_results.csv`):
    *   Градиентный спуск (GD) с различными стратегиями выбора шага (постоянный, экспоненциальное/ступенчатое затухание, поиск по Армихо и золотому сечению).
    *   Методы из библиотеки `SciPy` (квазиньютоновский BFGS, метод сопряженных градиентов CG).
    *   Стохастический градиентный спуск (SGD) из `Torch`.
*   Эвристические методы (данные из `summary_results.csv`):
    *   Метод восхождения на холм (Hill Climbing).
    *   Имитация отжига (Simulated Annealing).
    *   Генетический алгоритм (Genetic Algorithm).
    *   К этой же группе можно отнести метод Нелдера-Мида (`Nelder-Mead`) из `SciPy`.

Функции:

Использовать те же самые функции, что и первой лабораторной: 
- `Booth`, `Эллиптический параболоид`, `Вытянутая квадратичная долина`: Имеют один глобальный минимум, хорошо подходят для тестирования скорости сходимости.
- `Ackley`: Имеют множество локальных минимумов
- `booth_noisy`, `Зашумленная Функция Бута`: Позволяют оценить робастность (устойчивость) методов к неточным данным.

## 3. Анализ результатов

3.1. Производительность на простых унимодальных функциях (Бута, Сфера)

*   Градиентные методы: На этих функциях они показывают наилучшие результаты. Методы с адаптивным шагом (например, BFGS из `SciPy`) находят точное решение (`final_value` ~ 0.0) за минимальное число итераций (3-7 итераций) и вычислений функции/градиента (4-8 вычислений). Это на порядки эффективнее, чем у эвристических методов.
*   Эвристические методы: Они также находят решения, близкие к оптимальным. Однако их стоимость значительно выше. Например, `Hill Climbing` находит для функции Сферы решение с точностью `0.001`, но тратит на это 501 вычисление функции. Генетический алгоритм требует 550 вычислений. Это демонстрирует, что при отсутствии информации о градиенте поиск становится значительно дороже.

3.2. Производительность на сложных мультимодальных функциях (Экли, Растригина)

*   Градиентные методы: Здесь их эффективность резко падает. Они быстро находят ближайший минимум и застревают в нем, не имея механизма для глобального поиска. Так, на функции Экли многие градиентные методы сходятся к локальному оптимуму со значением `~6.45`, тогда как глобальный минимум равен 0.
*   Эвристические методы: В этих условиях они показывают свое главное преимущество. Генетический алгоритм и Имитация отжига демонстрируют способность находить решения, значительно более близкие к глобальному оптимуму. Например, при старте из центра для функции Экли все три эвристических метода нашли глобальный минимум со значением `~1e-16`. Это доказывает их применимость для задач глобальной оптимизации, хотя и ценой большого количества вычислений функции (501-550).

3.3. Производительность на зашумленных функциях

*   Градиентные методы: Шум в данных делает вычисление градиента ненадежным, что катастрофически сказывается на производительности. Многие градиентные методы либо не сходятся, либо показывают очень плохую точность.
*   Эвристические методы: Они гораздо более устойчивы к шуму. Имитация отжига, благодаря своему вероятностному механизму принятия худших решений, и Генетический алгоритм, работающий с целой популяцией, эффективно усредняют шум и продолжают поиск в верном направлении. Они стабильно находят хорошие решения, в то время как `Hill Climbing`, будучи жадным алгоритмом, более подвержен влиянию случайных выбросов.

#### 4. Выводы

Проведенное исследование наглядно демонстрирует фундаментальный компромисс в методах оптимизации.

1.  Градиентные методы (BFGS, CG, GD с адаптивным шагом) являются лучшим выбором для локальной оптимизации гладких, унимодальных функций. Они чрезвычайно эффективны по количеству вычислений, но не способны к глобальному поиску и чувствительны к шуму.

2.  Эвристические методы (Генетический алгоритм, Имитация отжига) незаменимы для задач глобальной оптимизации, а также при работе с зашумленными, недифференцируемыми или черными функциями. Платой за их робастность и универсальность является значительно более высокая вычислительная стоимость.

3.  Выбор метода должен основываться на характеристиках задачи:
    *   Если функция проста и ее градиент доступен, следует использовать градиентные методы.
    *   Если функция сложная, мультимодальная, или о ней мало что известно, предпочтение стоит отдать эвристическим методам. Среди них Генетический алгоритм лучше всего подходит для широкого исследования пространства поиска, а Имитация отжига является хорошим компромиссом между исследованием и скоростью сходимости.

Таким образом, не существует универсально лучшего алгоритма. Эффективность определяется правильным выбором инструмента для конкретной задачи. 